#import pandas, scipy and sklearn packages

import pandas as pd
import scipy.stats
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import tree
from itertools import repeat
import numpy as np

df = pd.read_csv('SupervisedLearning/supervised_test.csv')

#TODO: Write code below to inspect the first five rows of the data frame
df.head()

# Run this section to inspect X
X = df.drop(columns = ['genre'])

#TODO: Write code to inspect X
X

# Uncomment this section to inpect y
y = df['genre']

#TODO: Write code to inspect y
y

# Compute the maximum entropy value
k = y.unique().size
maxE = np.log2(k)
p_data = y.value_counts(normalize=True)           # counts occurrence of each value
entropy = scipy.stats.entropy(p_data)  # get entropy from counts

# normalize the value to be between 0 and 1.
normalizedE = entropy/maxE

#TODO: Write code to display the entropy value
entropy

avg_score = 0.0
ntimes = 30

for i in repeat(None, ntimes):

    # train model with 80% of the data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # prediction using entropy
    # Note: You can replace 'entropy' by 'gini' to get the classifier to use the gini index criterion.
    model = DecisionTreeClassifier(criterion='entropy')
    model.fit(X_train,y_train)
    predictions = model.predict(X_test)

    # compute model accuracy
    avg_score += accuracy_score(y_test, predictions)

avg_score /= ntimes

print('normalized entropy value: %.3f'% normalizedE)
print('average accuracy score: %.3f' % avg_score)


# output visual (can be visualized with visual code)
tree.export_graphviz(model, out_file='SupervisedLearning/EntropySupervisedModel.dot',
                    feature_names=['age', 'gender'],
                    class_names=sorted(y.unique()),
                    label='all',
                    rounded=True,
                    filled=True)

# Imports necessary packages
from sklearn import preprocessing

# Create new data frame
df = pd.read_csv('SupervisedLearning/waste water treatment.csv')

# #Write code below to inspect the first five rows of the data frame
df.head()

# Create input dataset
X = df

# Write code to inspect the shape of X
X.shape

# Clean input dataset
for i in range(1900, 3048): 
    X = X.drop(i)

for i in range(0, 1900):
    if X.VariableDescription[i] == "Unspecified (other) treatment ": 
        X = X.drop(i)

# Write code to inspect the shape of X
X.shape

# Create output dataset
y = X['Country']
# y = X.drop(columns = ['VariableDescription','Variable','Year','PercentageValue'])

# Write code to inspect y
y

# Create input and output datasets
X = X.drop(columns = ['Country'])
X = X.drop(columns = ['Variable'])

# Write code to inspect X
X

# Write code to inspect the shape of X
X.shape

# Write code to inspect the shape of y
y.shape

# X is expected to be an array here. If it's a dataframe, get the array version by running:
X = X.values

# Write code to inspect X
X

# Turns strings under "VariableDescription" column into integers
le_df = preprocessing.LabelEncoder()
# le_df.fit(['Primary treatment','Secondary treatment'])
le_df.fit(['Total public sewerage (% of resident population connected to urban wastewater collecting system = PUBTOTTR + PUBNOTR)','Connected to a wastewater treatment plant without treatment','Primary treatment','Secondary treatment','Tertiary treatment','Population connected to independent treatment','Total treatment ( = PUBTOTTR + INDEPDTR)','Unspecified (other) treatment','Not connected to public sewerage or independent treatment','Public total treatment (connected to a wastewater treatment plant = PUBMECTR + PUBBIOTR + PUBADVTR + OTHERTR)'])
X[:,0] = le_df.transform(X[:,0]) 

# Compute the maximum entropy value
k = y.unique().size
maxE = np.log2(k)
p_data = y.value_counts(normalize=True)           # counts occurrence of each value
entropy = scipy.stats.entropy(p_data)  # get entropy from counts

# normalize the value to be between 0 and 1.
normalizedE = entropy/maxE

# Write code to display the entropy value
entropy

avg_score = 0.0
ntimes = 30

for i in repeat(None, ntimes):

    # train model with 80% of the data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # prediction using entropy
    # Note: You can replace 'entropy' by 'gini' to get the classifier to use the gini index criterion.
    model = DecisionTreeClassifier(criterion='entropy')
    model.fit(X_train,y_train)
    predictions = model.predict(X_test)

    # compute model accuracy
    avg_score += accuracy_score(y_test, predictions)

avg_score /= ntimes

print('normalized entropy value: %.3f'% normalizedE)
print('average accuracy score: %.3f' % avg_score)

# output visual (can be visualized with visual code)
tree.export_graphviz(model, out_file='SupervisedLearning/waste_water_treatment_entropy.dot',
                    feature_names=['VariableDescription', 'PercentageValue', 'Year'],
                    class_names=sorted(y.unique()),
                    label='all',
                    rounded=True,
                    filled=True)
